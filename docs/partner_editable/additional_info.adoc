As shown in <<cfn_outputs>>, the AWS CloudFormation template creates the following resources on the *Outputs* tab:

* SageMakerTableauApi: The URL for users to connect to the deployment from Tableau.
* UserPoolDomain: The Amazon Cognito URL to sign up and sign in users of the deployment.

On the *Resources* tab, you find the SolutionSG security group. This security group is public by default. After testing the deployment, edit the inbound and outbound rules of the SolutionSG security group as necessary to ensure they conform to your VPC security policies. For more information, refer to https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/working-with-security-groups.html[Work with security groups].

=== Step 3. Test the Deployment

To test the deployment, navigate to the UserPoolDomain URL displayed in the *Outputs* tab, and sign up as a user. Then, test signing in using the new credentials.

=== Step 4. (Optional) Testing from Tableau (Version 2020.1 or later)

[start=1]
. In Tableau Desktop, choose *Help*, *Settings & Performance*, *Manage Analytics Extension Connection*. 
. For *Select an Analytics Extension*, choose *TabPy/External API*.
. For *Server*, ... .
. For *Port*, enter *443*.
. Select *Sign in with a username and password*, then enter your credentials in the fields provided.
. Select *Require SSL*.
. Choose *Test Connection*.
. Click *OK*. If successful, the message *Successfully connected to the analytics extension* displays. If unsuccessful, an error message displays.

[#tableau2]
.Analytics Extension Connection dialog box
[link=images/tableau_connection.png]
image::../images/tableau_connection.png[TableauConnection]

= Additional Information

== Best practices for using {partner-product-name} on AWS
// Provide post-deployment best practices for using the technology on AWS, including considerations such as migrating data, backups, ensuring high performance, high availability, etc. Link to software documentation for detailed information.

While using the {partner-product-name} deployment, it is important to follow best practices for Tableau Desktop and SageMaker. Tableau users can use any Amazon SageMaker hosted ML model in Tableau table calculations. However, users should pass data from Tableau table calculations to the analytics extension at the granularity level expected by the model (for example, with no aggregation or translation).

The {partner-product-name} deployment can be called from Tableau SCRIPT_ functions (SCRIPT_REAL, SCRIPT_STR, SCRIPT_INT, SCRIPT_BOOL), available inside of Tableau calculated fields. With SCRIPT_ functions, users can pass a script along with a block of data to an external analytics engine. The syntax for these calculations is as follows:

`Script_Function (‘[SageMaker Hosted Endpoint]’, <fields in dataset to pass to model>)` 

[#tableau_additionalinfo]
.Calculation syntax for mapping a data source in Tableau to the input schema of an Amazon SageMaker hosted model
[link=images/tableau_calculations.png]
image::../images/tableau_calculations.png[TableauCalc]

[NOTE]
====
- *Script Functions*. The function you use in your calculated field must match the return data type of your SageMaker model.
- *SageMaker Hosted Endpoint*. The SageMaker model must have a hosted endpoint.
- *Fields to pass to model*. You must pass each field in the dataset from Tableau, in the order that the SageMaker model is expecting.
====

== Customization

It is recommended for users of the {partner-product-name} solution to use Amazon SageMaker Autopilot to train models, in the event that you are not using an Autopilot trained model it is possible the solution will require customizations. The solution utilizes a Lambda function to translate the Tableau Analytics Extension API call to a format that is compatible with SageMaker endpoints trained with AutoPilot. In the event that you need to apply customizations to the solution, follow these steps.

Tableau sends data from the extension in the following format:

.Tableau analytics extension data format
[source,json]
----
{'_arg1': [37, 40, 56, 45, 46, 55, 52, 45], '_arg2': ['services', 'admin.', 'services', 'services', 'blue-collar', 'retired', 'technician', 'blue-collar'], '_arg3': ['married', 'married', 'married', 'married', 'married', 'single', 'married', 'married'], '_arg4': ['high.school', 'basic.6y', 'high.school', 'basic.9y', 'basic.6y', 'high.school', 'basic.9y', 'basic.9y'], '_arg5': ['no', 'no', 'no', 'unknown', 'unknown', 'no', 'no', 'no'], '_arg6': ['yes', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes'], '_arg7': ['no', 'no', 'yes', 'no', 'yes', 'no', 'no', 'no'], '_arg8': ['telephone', 'telephone', 'telephone', 'telephone', 'telephone', 'telephone', 'telephone', 'telephone'], '_arg9': ['may', 'may', 'may', 'may', 'may', 'may', 'may', 'may'], '_arg10': ['mon', 'mon', 'mon', 'mon', 'mon', 'mon', 'mon', 'mon'], '_arg11': [226, 151, 307, 198, 440, 342, 1666, 225], '_arg12': [1, 1, 1, 1, 1, 1, 1, 2], '_arg13': [999, 999, 999, 999, 999, 999, 999, 999], '_arg14': [0, 0, 0, 0, 0, 0, 0, 0], '_arg15': ['nonexistent', 'nonexistent', 'nonexistent', 'nonexistent', 'nonexistent', 'nonexistent', 'nonexistent', 'nonexistent'], '_arg16': [1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1, 1.1], '_arg17': [93.994, 93.994, 93.994, 93.994, 93.994, 93.994, 93.994, 93.994], '_arg18': [-36.4, -36.4, -36.4, -36.4, -36.4, -36.4, -36.4, -36.4], '_arg19': [4.857, 4.857, 4.857, 4.857, 4.857, 4.857, 4.857, 4.857], '_arg20': [5191, 5191, 5191, 5191, 5191, 5191, 5191, 5191]}
----

In the CloudFormation pane showing the solution's resources, navigate to the Evaluate Lambda Function. 

The Evaluate Lambda Function is authored in Python 3.7, and contains a function titled "create_sagemaker_body". This function facilitates transformation of Tableau's JSON, into the following output (text/csv) to be sent to the Amazon SageMaker endpoint. 

.Formatted data for SageMaker AutoPilot trained model
[source,csv]
----
37,services,married,high.school,no,yes,no,telephone,may,mon,226,1,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191
40,admin.,married,basic.6y,no,no,no,telephone,may,mon,151,1,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191
56,services,married,high.school,no,no,yes,telephone,may,mon,307,1,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191
45,services,married,basic.9y,unknown,no,no,telephone,may,mon,198,1,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191
46,blue-collar,married,basic.6y,unknown,yes,yes,telephone,may,mon,440,1,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191
55,retired,single,high.school,no,yes,no,telephone,may,mon,342,1,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191
52,technician,married,basic.9y,no,yes,no,telephone,may,mon,1666,1,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191
45,blue-collar,married,basic.9y,no,yes,no,telephone,may,mon,225,2,999,0,nonexistent,1.1,93.994,-36.4,4.857,5191
----

In the event that your ML model requires additional transformations, it is not recommended to modify the Lambda functions code. It is best practice to package input pre-processing logic alongside the ML model as an Amazon SageMaker inference pipeline as described in https://aws.amazon.com/blogs/machine-learning/preprocess-input-data-before-making-predictions-using-amazon-sagemaker-inference-pipelines-and-scikit-learn/[this blog^]. The input pre-processing logic is responsible for transforming the data format that the Lambda function sends to the SageMaker model endpoint into the format required by the ML model. This allows for additional transformations and presents the opportunity to easily integrate customizations into the functionality of the {partner-product-name} solution. 

== References

=== AWS services
 * http://aws.amazon.com/documentation/acm/[AWS Certificate Manager]
* http://aws.amazon.com/documentation/cloudformation/[AWS CloudFormation]
* https://docs.aws.amazon.com/apigateway/[Amazon API Gateway]
* https://docs.aws.amazon.com/lambda/[AWS Lambda]
* https://docs.aws.amazon.com/cognito/[Amazon Cognito]
* https://docs.aws.amazon.com/sagemaker/[Amazon SageMaker]
* https://docs.aws.amazon.com/cognito/[Amazon Cognito]
* https://docs.aws.amazon.com/route53/[Amazon Route 53]

=== Quick Start reference deployments
 - https://aws.amazon.com/quickstart/[AWS Quick Start home page]

=== GitHub repository
You can visit our GitHub repository to download the templates and scripts for this Quick Start, to post your comments, and to share your customizations with others.